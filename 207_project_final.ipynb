{
 "metadata": {
  "name": "",
  "signature": "sha256:4dfd8c24da8985f4231393006b82d98467bd4f8933159cf4baf445717ee67584"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "207 Final Project: Random Acts of Pizza"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "By Gopala Tumuluri and Jason Goodman"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Monday, April 20, 2015"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Table of Contents **\n",
      "\n",
      "1. Problem Description  \n",
      "2. Project Steps  \n",
      "2a. Create dense features  \n",
      "2b. Create text features  \n",
      "2c. Create subreddit features  \n",
      "2d. Create vectorized text feature  \n",
      "2e. Make final prediction  \n",
      "  \n",
      "3. Error Analysis\n",
      "4. Summary Table  \n",
      "5. Discussion  \n",
      "\n",
      "6. Appendix  \n",
      "6a. LDA  \n",
      "6b. Doc2Vec"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 1. Problem Description **  \n",
      "Reddit's Random Acts of Pizza subreddit is a place where people can create posts requesting free pizza from others on the Internet. The object of this competition is simple: predict which requests will be met with success based on the features of those posts. Kaggle supplied a training dataset containing three types of features: metadata about the post and the poster, which we call 'dense features', a list of subreddits that the user subscribed to at the time of posting, and, of course, the text of the request itself. We handle each separately and blend them into a final model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 2. Project Steps **"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's load in our data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy\n",
      "import datetime\n",
      "from sklearn import svm\n",
      "from sklearn import tree\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import itemgetter, TfidfVectorizer\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from nltk import word_tokenize    \n",
      "from nltk.tag import pos_tag  \n",
      "import lda\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
      "import json\n",
      "import pprint\n",
      "import random\n",
      "import re\n",
      "import csv\n",
      "\n",
      "### Read the JSON files into a list of dictionaries, with each JSON document read as a dictionary.\n",
      "def readjson(filename):\n",
      "    with open(filename) as infile:\n",
      "        data = json.load(infile)\n",
      "    return data\n",
      "\n",
      "### Read train and test data.\n",
      "train = readjson(\"train.json\")\n",
      "test = readjson(\"test.json\")\n",
      "\n",
      "train_df = pd.DataFrame(train)\n",
      "test_df = pd.DataFrame(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset has approximately 25% successes and 75% failures. In order to improve performance, we'd like to balance our features so that it's closer to 50-50. One basic way of doing so without removing any data is to simply dupicate our successes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Double the successes to 'balance' the dataset\n",
      "train_df_fail = train_df[train_df['requester_received_pizza'] == 0]\n",
      "train_df_success = train_df[train_df['requester_received_pizza'] == 1]\n",
      "train_df = pd.concat([train_df_fail, train_df_success,\n",
      "                      train_df_success, train_df_success], axis = 0)\n",
      "train_df.index = range(train_df.shape[0])\n",
      "\n",
      "# Extract output labels separately for use in training and prediction\n",
      "# accuracy comparision on dev data.\n",
      "train_labels = np.ravel(train_df[['requester_received_pizza']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 2a. Dense Features **  \n",
      "Next, we create the 'dense' features on both the training and test sets. These mostly leverage the metadata given to us with the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_features(df):\n",
      "    # Length of the post's title\n",
      "    df['title_len'] = df.request_title.str.len()\n",
      "\n",
      "    # Length of the post's text\n",
      "    df['request_len'] = df.request_text_edit_aware.str.len()\n",
      "\n",
      "    # Whether or not the user was subscribed to exactly 0 subreddits\n",
      "    df['zero_subreddits'] = df['requester_number_of_subreddits_at_request'] == 0\n",
      "\n",
      "    # Total number of comments + number of random acts of pizza comments\n",
      "    df['total_comments'] = (df['requester_number_of_comments_in_raop_at_request'] +\n",
      "                                 df['requester_number_of_comments_at_request'])\n",
      "\n",
      "    # The ratio of the user's total comments to their number of random acts of pizza comments\n",
      "    df['comment_ratio'] = (df['total_comments'] /\n",
      "                                df['requester_number_of_comments_in_raop_at_request'])\n",
      "    \n",
      "    df.loc[df['comment_ratio'] == np.inf, 'comment_ratio'] = \\\n",
      "        df.loc[df['comment_ratio'] != np.inf, 'comment_ratio'].mean(skipna = True)\n",
      "    \n",
      "    df.loc[pd.isnull(df['comment_ratio']), 'comment_ratio'] = \\\n",
      "        df.loc[pd.notnull(df['comment_ratio']), 'comment_ratio'].mean(skipna = True)\n",
      "\n",
      "    # The number of upvotes they've received\n",
      "    df['upvotes'] = (df['requester_upvotes_minus_downvotes_at_request'] +\n",
      "                       df['requester_upvotes_plus_downvotes_at_request']) / 2\n",
      "\n",
      "    # The number of downvotes they've received\n",
      "    df['downvotes'] = (df['requester_upvotes_plus_downvotes_at_request'] -\n",
      "                             df['upvotes'])\n",
      "    \n",
      "    # The ratio of upvotes they've received\n",
      "    df['upvote_ratio'] = (df['upvotes'] /\n",
      "                               (df['upvotes'] + df['downvotes']))\n",
      "    \n",
      "    df.loc[df['upvote_ratio'] == np.inf, 'upvote_ratio'] = \\\n",
      "        df.loc[df['upvote_ratio'] != np.inf, 'upvote_ratio'].mean(skipna = True)\n",
      "\n",
      "    df.loc[pd.isnull(df['upvote_ratio']), 'upvote_ratio'] = \\\n",
      "        df.loc[pd.notnull(df['upvote_ratio']), 'upvote_ratio'].mean(skipna = True)\n",
      "\n",
      "    # Get the date in order to make future variables\n",
      "    df['date'] = pd.to_datetime(df['unix_timestamp_of_request_utc'], unit = 's')\n",
      "\n",
      "    # Hour of the post\n",
      "    df['hour'] = pd.DatetimeIndex(df['date']).hour\n",
      "\n",
      "    # Day of the post\n",
      "    df['day'] = pd.DatetimeIndex(df['date']).day\n",
      "\n",
      "    # The post's day of the week\n",
      "    df['weekday'] = pd.DatetimeIndex(df['date']).weekday\n",
      "\n",
      "    # Whether the post was made in the first half of the month\n",
      "    df['first_half_of_month'] = df['day'] <= 15\n",
      "    df['first_half_of_month'] = df['first_half_of_month'].astype(int)\n",
      "\n",
      "    # Whether the post was made on a weekend\n",
      "    df['weekend'] = (df['weekday'] == 5) | (df['weekday'] == 6)\n",
      "    df['weekend'] = df['weekend'].astype(int)\n",
      "\n",
      "    # Whether the post was made in the morning\n",
      "    df['morning'] = (df['hour'] >= 6) & (df['hour'] < 12)\n",
      "    df['morning'] = df['morning'].astype(int)\n",
      "\n",
      "    # Whether the post was made in the afternoon\n",
      "    df['afternoon'] = (df['hour'] >= 12) & (df['hour'] < 16)\n",
      "    df['afternoon'] = df['afternoon'].astype(int)\n",
      "\n",
      "    # Whether the post was made in the evening\n",
      "    df['evening'] = (df['hour'] >= 16) & (df['hour'] < 20)\n",
      "    df['evening'] = df['evening'].astype(int)\n",
      "\n",
      "    # Whether the post was made at night\n",
      "    df['night'] = (df['hour'] >= 20) & (df['hour'] < 23)\n",
      "    df['night'] = df['night'].astype(int)\n",
      "\n",
      "    # Whether the post was made late at night\n",
      "    df['latenight'] = (df['hour'] >= 23) & (df['hour'] < 6)\n",
      "    df['latenight'] = df['latenight'].astype(int)\n",
      "\n",
      "    # Whether there was a difference between the utc and unix timestamp\n",
      "    df['utcdiff'] = (df['unix_timestamp_of_request_utc'] -\n",
      "                              df['unix_timestamp_of_request'])\n",
      "\n",
      "    # Month of the post\n",
      "    df['month'] = pd.DatetimeIndex(df['date']).month\n",
      "\n",
      "    # Week of the post\n",
      "    df['week'] = pd.DatetimeIndex(df['date']).week\n",
      "    \n",
      "    # Get the US federal holidays and categorize the request dates as holiday or not.\n",
      "    cal = USFederalHolidayCalendar()\n",
      "    holiday_list = cal.holidays(start=df['date'].min(),\n",
      "                            end=df['date'].max())\n",
      "    holiday_list = [time.date() for time in holiday_list]\n",
      "\n",
      "    df['justdate'] = [time.date() for time in df['date']]\n",
      "    df['holiday'] = df['justdate'].isin(holiday_list)\n",
      "    df['holiday'] = df['holiday'].astype(int)\n",
      "\n",
      "    return(df)\n",
      "\n",
      "train_df = create_features(train_df)\n",
      "test_df = create_features(test_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 2b. Text Features **  \n",
      "Next up, we create features out of the text and add them to the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Extract the user request text so that we can create word features out\n",
      "### of it for inclusion in the training since we can't use whole text.\n",
      "train_request_text = train_df[['request_text_edit_aware']]\n",
      "test_request_text = test_df[['request_text_edit_aware']]\n",
      "\n",
      "### Code to add in text features\n",
      "def create_text_features(df):\n",
      "    # Training data with engineered features\n",
      "    df_new = df \n",
      "\n",
      "    # Number of words in title\n",
      "    df_new['title_nwords'] = df['request_title'].apply(lambda x: len(x.split(' ')))\n",
      "\n",
      "    # Number of words in body\n",
      "    df_new['body_nwords'] = df['request_text_edit_aware'].apply(lambda x: len(x.split(' ')))\n",
      "    \n",
      "    # Number of sentences in body\n",
      "    sen_count = re.compile(r'([a-zA-Z][^\\.!?]*[\\.!?])', re.M)\n",
      "    df_new['body_sents'] = df['request_text_edit_aware'].apply(lambda x: len(sen_count.findall(x)))\n",
      "\n",
      "    # Post includes an image\n",
      "    image_count = re.compile(r'(imgur\\.com|\\.jpg)', re.IGNORECASE)\n",
      "    images = df['request_text_edit_aware'].apply(lambda x: len(image_count.findall(x)))\n",
      "    df_new['has_image'] = np.where(images > 0,1,0)\n",
      "\n",
      "    # Post includes a 'tl;dr'\n",
      "    tldr_count = re.compile(r'(tl;dr|tldr|tl,dr|tl:dr)', re.IGNORECASE)\n",
      "    tldrs = df['request_text_edit_aware'].apply(lambda x: len(tldr_count.findall(x)))\n",
      "    df_new['has_tldr'] = np.where(tldrs > 0,1,0)\n",
      "\n",
      "    # More than zero all-caps words in title (min char = 3)\n",
      "    df_new['title_caps'] = df['request_title'].apply( \\\n",
      "        lambda x: np.where(sum(1 for c in re.split('\\W+',x) if c.isupper() and len(c) > 3) > 0, 1,0))\n",
      "\n",
      "    # More than zero all-caps words in body (min char = 2)\n",
      "    df_new['body_caps'] = df['request_text_edit_aware'].apply( \\\n",
      "        lambda x: np.where(sum(1 for c in re.split('\\W+',x) if c.isupper() and len(c) > 2) > 0, 1,0))\n",
      "\n",
      "    # Exclamation marks in title\n",
      "    excl_marks = re.compile(r'(!)', re.M)\n",
      "    df_new['excl_marks_title'] = df['request_title'].apply(lambda x: len(excl_marks.findall(x)))\n",
      "    # print train_df_f['excl_marks']\n",
      "\n",
      "    # Exclamation marks in body\n",
      "    df_new['excl_marks_body'] = df['request_text_edit_aware'].apply(lambda x: len(excl_marks.findall(x)))\n",
      "\n",
      "    # Dollar signs in title\n",
      "    dollar_signs = re.compile(r'(\\$|dollar)', re.IGNORECASE)\n",
      "    df_new['dollars_title'] = df['request_title'].apply(lambda x: len(dollar_signs.findall(x)))\n",
      "\n",
      "    # Dollar signs in body\n",
      "    df_new['dollars_body'] = df['request_text_edit_aware'].apply(lambda x: len(dollar_signs.findall(x)))\n",
      "    \n",
      "    # Categories\n",
      "    desire = re.compile(r'(friend|party|birthday|boyfriend|girlfriend|date|drinks|drunk|wasted|invite|invited|celebrate|celebrating|game|games|movie|beer|crave|craving)', re.IGNORECASE)\n",
      "    family = re.compile(r'(husband|wife|family|parent|parents|mother|father|mom|mum|son|dad|daughter)', re.IGNORECASE)\n",
      "    job = re.compile(r'(job|unemployment|employment|hire|hired|fired|interview|work|paycheck)', re.IGNORECASE)\n",
      "    money = re.compile(r'(money|bill|bills|rent|bank|account|paycheck|due|broke|bills|deposit|cash|dollar|dollars|bucks|paid|payed|buy|check|spent|financial|poor|loan|credit|budget|day|now| \\\n",
      "        time|week|until|last|month|tonight|today|next|night|when|tomorrow|first|after|while|before|long|hour|Friday|ago|still|due|past|soon|current|years|never|till|yesterday|morning|evening)', re.IGNORECASE)\n",
      "    student = re.compile(r'(college|student|university|finals|study|studying|class|semester|school|roommate|project|tuition|dorm)', re.IGNORECASE)\n",
      "    \n",
      "    df_new['desire_category'] = df['request_text_edit_aware'].apply(lambda x: len(desire.findall(x)))\n",
      "    df_new['family_category'] = df['request_text_edit_aware'].apply(lambda x: len(family.findall(x)))\n",
      "    df_new['job_category'] = df['request_text_edit_aware'].apply(lambda x: len(job.findall(x)))\n",
      "    df_new['money_category'] = df['request_text_edit_aware'].apply(lambda x: len(money.findall(x)))\n",
      "    df_new['student_category'] = df['request_text_edit_aware'].apply(lambda x: len(student.findall(x)))\n",
      "    \n",
      "    # Gratitude\n",
      "    gratitude = re.compile(r'(thank|thanks|thankful|appreciate|grateful|gratitude|advance)',re.IGNORECASE)\n",
      "    df_new['gratitude'] = df['request_text_edit_aware'].apply(lambda x: np.where(len(gratitude.findall(x)) > 0, 1, 0))\n",
      "    \n",
      "    # Reciprocity\n",
      "    reciprocity = re.compile(r'(pay it forward|pay forward|paid it forward|pay the act forward|pay the favor back|paying it forward|pay this forward|pay pizza forward|pay back|pay it back|pay you back|return the favor|return the favour|pay a pizza forward|repay)',re.IGNORECASE)    \n",
      "    df_new['reciprocity'] = df['request_text_edit_aware'].apply(lambda x: np.where(len(reciprocity.findall(x))> 0, 1, 0))\n",
      "    \n",
      "    return(df_new)\n",
      "\n",
      "train_df = create_text_features(train_df)\n",
      "test_df = create_text_features(test_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having made the text features, we now turn categorical variables into dummy variables and throw away variables we won't use anymore."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_categorical = train_df[['hour', 'week', 'day', 'weekday', 'month', 'utcdiff']]\n",
      "test_data_categorical = test_df[['hour', 'week', 'day', 'weekday', 'month', 'utcdiff']]\n",
      "\n",
      "train_data = train_df.drop(['request_text', 'requester_received_pizza',\n",
      "                            'giver_username_if_known', 'post_was_edited',\n",
      "                            'request_id', 'request_text_edit_aware',\n",
      "                            'request_title',\n",
      "                            'requester_subreddits_at_request',\n",
      "                            'unix_timestamp_of_request_utc',\n",
      "                            'unix_timestamp_of_request',\n",
      "                            'requester_username', 'requester_user_flair',\n",
      "                            'number_of_downvotes_of_request_at_retrieval',\n",
      "                            'number_of_upvotes_of_request_at_retrieval',\n",
      "                            'request_number_of_comments_at_retrieval',\n",
      "                            'requester_account_age_in_days_at_retrieval',\n",
      "                            'requester_days_since_first_post_on_raop_at_retrieval',\n",
      "                            'requester_number_of_comments_at_retrieval',\n",
      "                            'requester_number_of_comments_in_raop_at_retrieval',\n",
      "                            'requester_number_of_posts_at_retrieval',\n",
      "                            'requester_number_of_posts_on_raop_at_retrieval',\n",
      "                            'requester_upvotes_minus_downvotes_at_retrieval',\n",
      "                            'requester_upvotes_plus_downvotes_at_retrieval',\n",
      "                            'date', 'hour', 'week', 'day', 'weekday', 'month', 'utcdiff', 'justdate'\n",
      "                            ],\n",
      "                           axis = 1)\n",
      "\n",
      "test_data = test_df.drop(['giver_username_if_known', 'request_id',\n",
      "                          'request_text_edit_aware',\n",
      "                            'request_title',\n",
      "                            'requester_subreddits_at_request',\n",
      "                            'unix_timestamp_of_request_utc',\n",
      "                            'unix_timestamp_of_request',\n",
      "                            'requester_username',\n",
      "                            'date', 'hour', 'week', 'day', 'weekday', 'month', 'utcdiff', 'justdate'\n",
      "                            ],\n",
      "                           axis = 1)\n",
      "\n",
      "train_data_dummy = pd.DataFrame(index = train_data.index)\n",
      "test_data_dummy = pd.DataFrame(index = test_data.index)\n",
      "\n",
      "for column in train_data_categorical.columns:\n",
      "    train_data_dummy = pd.concat([train_data_dummy,\n",
      "                                  pd.get_dummies(train_data_categorical[column],\n",
      "                                                prefix = column)], axis = 1)    \n",
      "    test_data_dummy = pd.concat([test_data_dummy,\n",
      "                                 pd.get_dummies(test_data_categorical[column],\n",
      "                                                prefix = column)], axis = 1)\n",
      "\n",
      "train_data = pd.concat([train_data, train_data_dummy], axis = 1)\n",
      "test_data = pd.concat([test_data, test_data_dummy], axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 2c. Subreddit Features **  \n",
      "Users are subscribed to different sets of subreddits. We turn these lists into a long list of dummy variables, and then use PCA in order to limit the list to a reasonable number of features while still preserving most of the variation. We train a logistic model on these features, and then save the resulting prediction probabilities for each training and test observation as a new feature entitled \"subreddit_proba\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The code below creates a subreddit frequency dictionary, and also a\n",
      "# dictionary of lists for each subreddit with 0/1 values for each observation\n",
      "# in the training and test dataset. This dict of lists is basically the\n",
      "# dummy coding for each subreddit name, and will be turned into a pandas\n",
      "# dataframe. \n",
      "def get_subreddit_user_list_train(df, colname):\n",
      "    subreddit_freq = {}\n",
      "    subreddit_user_list = {}\n",
      "    index = 0\n",
      "    for subreddit_list in list(df[colname]):\n",
      "        for subreddit in subreddit_list:\n",
      "            if subreddit in subreddit_freq:\n",
      "                subreddit_freq[subreddit] += 1\n",
      "            else:\n",
      "                subreddit_freq[subreddit] = 1\n",
      "                subreddit_user_list[subreddit] = []\n",
      "            for i in range(len(subreddit_user_list[subreddit]), index):\n",
      "                subreddit_user_list[subreddit].append(0)\n",
      "            subreddit_user_list[subreddit].append(1)\n",
      "        index += 1\n",
      "    return subreddit_freq, subreddit_user_list\n",
      "\n",
      "# This function does the same process above, but for the test data. The function is\n",
      "# different because it will NOT create new subreddit features that don't exist in\n",
      "# the train data set. Important for proper training.\n",
      "def get_subreddit_user_list_test(df, colname, subreddit_freq):\n",
      "    subreddit_user_list = {}\n",
      "    index = 0\n",
      "\n",
      "    for subreddit in subreddit_freq:\n",
      "        subreddit_user_list[subreddit] = []\n",
      "\n",
      "    for subreddit_list in list(df[colname]):\n",
      "        for subreddit in subreddit_list:\n",
      "            if subreddit in subreddit_user_list:\n",
      "                subreddit_freq[subreddit] += 1\n",
      "                for i in range(len(subreddit_user_list[subreddit]), index):\n",
      "                    subreddit_user_list[subreddit].append(0)\n",
      "                subreddit_user_list[subreddit].append(1)\n",
      "        index += 1\n",
      "    return subreddit_freq, subreddit_user_list\n",
      "\n",
      "# Fill with zeroes to create a full table of values for it to be used in\n",
      "# the dataframe. In other words, make the matrix dense.\n",
      "def subreddit_fill_zeroes(subreddit_user_list, size):\n",
      "    for subreddit in subreddit_user_list:\n",
      "        for i in range(len(subreddit_user_list[subreddit]), size):\n",
      "            subreddit_user_list[subreddit].append(0)\n",
      "\n",
      "# Create the subreddit columns for training data\n",
      "# Turn into a pandas dataframe based on the training data.\n",
      "subreddit_freq, subreddit_user_list = get_subreddit_user_list_train(train_df,\n",
      "                                          'requester_subreddits_at_request')\n",
      "subreddit_fill_zeroes(subreddit_user_list, train_df.shape[0])\n",
      "\n",
      "train_subreddit_user_list = pd.DataFrame(subreddit_user_list,\n",
      "                                         index = range(train_df.shape[0]))\n",
      "\n",
      "# Use the subreddit frequency table to create the dummy value columns for\n",
      "# test data. We will only use subreddits that already existed in the train\n",
      "# data above, and will ignore new subreddits that only exist in test data.\n",
      "# Turn the result into a pandas dataframe based on test data.\n",
      "subreddit_freq, subreddit_user_list = get_subreddit_user_list_test(test_df,\n",
      "                                          'requester_subreddits_at_request',\n",
      "                                          subreddit_freq)\n",
      "subreddit_fill_zeroes(subreddit_user_list, test_df.shape[0])\n",
      "test_subreddit_user_list = pd.DataFrame(subreddit_user_list,\n",
      "                                        index = range(test_df.shape[0]))\n",
      "\n",
      "# Perform PCA on the \"subreddit\" features only. Selected 260 as the number of\n",
      "# components based on experimentation, which revealed that about 70% of variance\n",
      "# is explained by these many components.\n",
      "pca = PCA(n_components = 260)\n",
      "train_data_pca = pca.fit_transform(train_subreddit_user_list)\n",
      "test_data_pca = pca.transform(test_subreddit_user_list)\n",
      "\n",
      "# Train a logistic regression model.\n",
      "logistic = LogisticRegression()\n",
      "logistic.fit(train_data_pca, train_labels)\n",
      "\n",
      "# Predict the probabilities for the training and test data sets for both the classes,\n",
      "# and concatenate these probabilities as a new structured feature into the original\n",
      "# dataframe consisting of other structured features and corresponding dummy variables\n",
      "# for the categorical data.\n",
      "predictions = logistic.predict_proba(train_data_pca)\n",
      "train_data_subreddit = pd.concat([train_data,\n",
      "                                 pd.DataFrame({'subreddit_proba': predictions[:, 0]})],\n",
      "                                 axis = 1)\n",
      "predictions = logistic.predict_proba(test_data_pca)\n",
      "test_data_subreddit = pd.concat([test_data,\n",
      "                                 pd.DataFrame({'subreddit_proba': predictions[:, 0]})],\n",
      "                                 axis = 1)\n",
      "\n",
      "# Retrain a logistic regression model with the structured features (including\n",
      "# probabilities from subreddit only training above). See how many times class\n",
      "# 1 is predicted.\n",
      "logistic = LogisticRegression()\n",
      "logistic.fit(train_data_subreddit, train_labels)\n",
      "predictions = logistic.predict_proba(test_data_subreddit.astype(float))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 2d. Vectorized Text Features **  \n",
      "In order to take advantage of the request text, we train a classifier based on the words present in the text. We extract the words, removing English stopwords, vectorize each request, and then perform PCA on the text in order to limit the number of variables in our model. We then train this feature set on a logistic regression, and add the resulting probabilities to our dataset in the same fashion as before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract text features from the post text by generating features from words\n",
      "vectorizer = CountVectorizer(stop_words = 'english')\n",
      "train_count_vectors = vectorizer.fit_transform(\n",
      "                          train_request_text['request_text_edit_aware'].values)\n",
      "test_count_vectors = vectorizer.transform(\n",
      "                         test_request_text['request_text_edit_aware'].values)\n",
      "\n",
      "# Turn the above sparse matrix representation into dense, as PCA expects\n",
      "train_count_vectors = train_count_vectors.toarray()\n",
      "test_count_vectors = test_count_vectors.toarray()\n",
      "\n",
      "# Performed experimenation with PCA and settled on 500 features as they captured\n",
      "# 80% of the variance of the text features.\n",
      "pca = PCA(n_components = 500)\n",
      "train_count_vectors_pca = pca.fit_transform(train_count_vectors)\n",
      "test_count_vectors_pca = pca.transform(test_count_vectors)\n",
      "\n",
      "# Train a logistic regression model on just the text features from post text.\n",
      "logistic = LogisticRegression()\n",
      "logistic.fit(train_count_vectors_pca, train_labels)\n",
      "\n",
      "# Predict the probabilities for the training and test data sets for both the classes,\n",
      "# and concatenate these probabilities as a new structured feature into the original\n",
      "# dataframe consisting of other structured features and corresponding dummy variables\n",
      "# for the categorical data.\n",
      "predictions = logistic.predict_proba(train_count_vectors_pca)\n",
      "train_data_subreddit_count = pd.concat([train_data_subreddit,\n",
      "                                 pd.DataFrame({'text_proba': predictions[:, 0]})],\n",
      "                                 axis = 1)\n",
      "predictions = logistic.predict_proba(test_count_vectors_pca)\n",
      "test_data_subreddit_count = pd.concat([test_data_subreddit,\n",
      "                                 pd.DataFrame({'text_proba': predictions[:, 0]})],\n",
      "                                 axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 2e. Final Predictions **  \n",
      "Now that we have all the features, we run them through a final logistic regression and output the resulting predictions.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train the final logistic regression model on the full set of features\n",
      "logistic = LogisticRegression()\n",
      "logistic.fit(train_data_subreddit_count, train_labels)\n",
      "predictions = logistic.predict(test_data_subreddit_count)\n",
      "\n",
      "# Turn predictions into a simple integer list for easy printing into a submission format\n",
      "predictions = [int(prediction) for prediction in list(predictions)]\n",
      "\n",
      "# Extract the request_id to incorporate into the submission file\n",
      "request_id = list(test_df['request_id'].values)\n",
      "\n",
      "# Create submission matrix\n",
      "data = [['request_id', 'requester_received_pizza']]\n",
      "for i in range(len(predictions)):\n",
      "    data.append([str(request_id[i]),str(predictions[i])])\n",
      "\n",
      "# Output to csv\n",
      "with open('final.csv', 'w') as fp:\n",
      "    a = csv.writer(fp, delimiter=',')\n",
      "    a.writerows(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 3. Error Analysis **  \n",
      "Throughout the process of determining the final model, we looked at which observations we were getting wrong in selecting features. The challenge is that our model tended to very well on cross-validated accuracy or f1 score, even exceeding 99% cross-validated accuracy at one point. However, it didn't perform as well on the actual test data. In general, we found that our models were predicting failure far too often. Several models we tried even predicted no successes at all in the test data. Rebalancing the training data by duplicating the successes helped considerably on this front - however, even our final submission only contained 555 successes out of 1631 test observations, when we know that the final test set was half successes and half failures. For a long time, our most accurate submission predicted only 213 success, effectively conceding most of the true positivies.\n",
      "\n",
      "In order to get a better sense of which posts we get wrong, we conducted the below analysis. We split our final dataset into training and test sets, ran a logistic regression on the training set, used it to predict the test set, and then compared the observations we predicted correctly versus those we predicted incorrectly. Unsurprisingly, the trend is that we are worse at predicting requests with less data: those with fewer words in their bodies, fewer upvotes, fewer category words, or fewer posts on the subreddit. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Divide final dataset into a train and test for analysis \n",
      "random.seed(0)\n",
      "random_split = [i for i in range(len(train_data_subreddit_count))]\n",
      "random.shuffle(random_split)\n",
      "\n",
      "train_set = sorted(random_split[0:(2 * len(train_data_subreddit_count)/3 + 1)])\n",
      "test_set = sorted(random_split[(2 * len(train_data_subreddit_count)/3 + 1):])\n",
      "\n",
      "train_data_err = train_data_subreddit_count.iloc[train_set,:]\n",
      "test_data_err = train_data_subreddit_count.iloc[test_set,:]\n",
      "\n",
      "train_labels_err = train_labels[train_set]\n",
      "test_labels_err = train_labels[test_set]\n",
      "\n",
      "# Run a logistic regression to see which factors correlate with prediction success\n",
      "logistic = LogisticRegression()\n",
      "logistic.fit(train_data_err, train_labels_err)\n",
      "predictions = logistic.predict(test_data_err)\n",
      "\n",
      "predicted_correctly = []\n",
      "accurate = 0\n",
      "for i in range(len(predictions)):\n",
      "    if predictions[i] == test_labels_err[i]:\n",
      "        accurate += 1\n",
      "        predicted_correctly.append(1)\n",
      "    else:\n",
      "        predicted_correctly.append(0)\n",
      "print \"Accuracy: \", round(accurate * 1.0 / len(predictions) * 100, 2)\n",
      "\n",
      "predicted_correctly = np.where(np.asarray(predicted_correctly) == 1,True,False)\n",
      "correct = test_data_err.loc[predicted_correctly,:]\n",
      "incorrect = test_data_err.loc[~predicted_correctly,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy:  75.96\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print out mean differences by variable of factors we get correct and incorrect\n",
      "# Divide by the standard deviation of each in the incorrect table to adjust for magnitude\n",
      "diffs = pd.concat([pd.DataFrame(correct.describe().loc['mean',:] - incorrect.describe().loc['mean',:]), \n",
      "           pd.DataFrame(incorrect.describe().loc['std',:])],axis=1)\n",
      "diffs['mean_div_std'] = diffs.loc[:,'mean'] / (1.0 * diffs.loc[:,'std'])\n",
      "diffs.sort(columns = 'mean_div_std',ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mean</th>\n",
        "      <th>std</th>\n",
        "      <th>mean_div_std</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>week_4</th>\n",
        "      <td>  0.00982962</td>\n",
        "      <td>          0</td>\n",
        "      <td>        inf</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>request_len</th>\n",
        "      <td>    114.7274</td>\n",
        "      <td>   259.8318</td>\n",
        "      <td>  0.4415451</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>body_nwords</th>\n",
        "      <td>     21.7493</td>\n",
        "      <td>   50.36154</td>\n",
        "      <td>  0.4318633</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
        "      <td>    2533.101</td>\n",
        "      <td>   6100.875</td>\n",
        "      <td>   0.415203</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>upvotes</th>\n",
        "      <td>    1497.989</td>\n",
        "      <td>   3628.885</td>\n",
        "      <td>  0.4127961</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>downvotes</th>\n",
        "      <td>    1035.112</td>\n",
        "      <td>   2560.231</td>\n",
        "      <td>  0.4043042</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>body_sents</th>\n",
        "      <td>    1.204266</td>\n",
        "      <td>   3.354955</td>\n",
        "      <td>  0.3589515</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
        "      <td>    462.8775</td>\n",
        "      <td>   1492.145</td>\n",
        "      <td>  0.3102095</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>money_category</th>\n",
        "      <td>    1.158471</td>\n",
        "      <td>   4.425892</td>\n",
        "      <td>  0.2617485</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>job_category</th>\n",
        "      <td>   0.2453701</td>\n",
        "      <td>   1.102057</td>\n",
        "      <td>  0.2226473</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>desire_category</th>\n",
        "      <td>   0.1462097</td>\n",
        "      <td>  0.6654951</td>\n",
        "      <td>  0.2197006</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>family_category</th>\n",
        "      <td>   0.1807036</td>\n",
        "      <td>  0.8623809</td>\n",
        "      <td>  0.2095403</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_6</th>\n",
        "      <td>  0.01606929</td>\n",
        "      <td> 0.07864736</td>\n",
        "      <td>  0.2043208</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_16</th>\n",
        "      <td>  0.01475868</td>\n",
        "      <td> 0.07864736</td>\n",
        "      <td>  0.1876563</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_18</th>\n",
        "      <td>  0.02220449</td>\n",
        "      <td>  0.1196342</td>\n",
        "      <td>  0.1856032</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>requester_number_of_comments_in_raop_at_request</th>\n",
        "      <td>    0.450662</td>\n",
        "      <td>   2.468582</td>\n",
        "      <td>   0.182559</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>excl_marks_body</th>\n",
        "      <td>   0.1470074</td>\n",
        "      <td>  0.8623958</td>\n",
        "      <td>   0.170464</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_22</th>\n",
        "      <td>  0.01530951</td>\n",
        "      <td>  0.0907195</td>\n",
        "      <td>  0.1687566</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_26</th>\n",
        "      <td>   0.0262693</td>\n",
        "      <td>  0.1558131</td>\n",
        "      <td>  0.1685949</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hour_8</th>\n",
        "      <td> 0.007103919</td>\n",
        "      <td> 0.04550158</td>\n",
        "      <td>  0.1561247</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>morning</th>\n",
        "      <td>  0.02288829</td>\n",
        "      <td>  0.1620032</td>\n",
        "      <td>   0.141283</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>has_tldr</th>\n",
        "      <td>  0.01082683</td>\n",
        "      <td> 0.07864736</td>\n",
        "      <td>  0.1376629</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>month_6</th>\n",
        "      <td>  0.03710563</td>\n",
        "      <td>  0.2758892</td>\n",
        "      <td>  0.1344947</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hour_1</th>\n",
        "      <td>   0.0300492</td>\n",
        "      <td>  0.2259164</td>\n",
        "      <td>  0.1330103</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_30</th>\n",
        "      <td>  0.02037153</td>\n",
        "      <td>  0.1558131</td>\n",
        "      <td>  0.1307434</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>requester_number_of_posts_on_raop_at_request</th>\n",
        "      <td>  0.03880563</td>\n",
        "      <td>   0.302605</td>\n",
        "      <td>  0.1282385</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>dollars_body</th>\n",
        "      <td>  0.04633692</td>\n",
        "      <td>  0.3711641</td>\n",
        "      <td>  0.1248422</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>zero_subreddits</th>\n",
        "      <td>  0.04015423</td>\n",
        "      <td>  0.3325248</td>\n",
        "      <td>  0.1207556</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_28</th>\n",
        "      <td>  0.01609778</td>\n",
        "      <td>  0.1353672</td>\n",
        "      <td>  0.1189194</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_44</th>\n",
        "      <td>  0.01072236</td>\n",
        "      <td>  0.0907195</td>\n",
        "      <td>  0.1181924</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>title_nwords</th>\n",
        "      <td>  -0.4484966</td>\n",
        "      <td>    7.90846</td>\n",
        "      <td>-0.05671099</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_34</th>\n",
        "      <td> -0.01019051</td>\n",
        "      <td>  0.1791519</td>\n",
        "      <td>-0.05688199</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_45</th>\n",
        "      <td>-0.007388835</td>\n",
        "      <td>  0.1277599</td>\n",
        "      <td>-0.05783374</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>month_5</th>\n",
        "      <td> -0.01597432</td>\n",
        "      <td>  0.2758892</td>\n",
        "      <td> -0.0579012</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>first_half_of_month</th>\n",
        "      <td> -0.02994473</td>\n",
        "      <td>   0.500277</td>\n",
        "      <td>-0.05985629</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weekday_3</th>\n",
        "      <td> -0.02256539</td>\n",
        "      <td>  0.3683645</td>\n",
        "      <td>-0.06125832</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>afternoon</th>\n",
        "      <td> -0.01835812</td>\n",
        "      <td>  0.2880458</td>\n",
        "      <td>-0.06373335</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>title_len</th>\n",
        "      <td>   -2.559557</td>\n",
        "      <td>   39.99822</td>\n",
        "      <td>-0.06399178</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_4</th>\n",
        "      <td> -0.01246985</td>\n",
        "      <td>  0.1945981</td>\n",
        "      <td>-0.06407998</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_18</th>\n",
        "      <td>-0.009668167</td>\n",
        "      <td>   0.149338</td>\n",
        "      <td>-0.06474018</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>holiday</th>\n",
        "      <td> -0.01226091</td>\n",
        "      <td>  0.1844677</td>\n",
        "      <td>-0.06646642</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_40</th>\n",
        "      <td> -0.01129219</td>\n",
        "      <td>  0.1679397</td>\n",
        "      <td>-0.06723956</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hour_18</th>\n",
        "      <td> -0.01859555</td>\n",
        "      <td>  0.2758892</td>\n",
        "      <td>-0.06740223</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_1</th>\n",
        "      <td> -0.01571789</td>\n",
        "      <td>  0.2217715</td>\n",
        "      <td>-0.07087429</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hour_14</th>\n",
        "      <td> -0.01281175</td>\n",
        "      <td>  0.1791519</td>\n",
        "      <td>-0.07151333</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>evening</th>\n",
        "      <td> -0.03271791</td>\n",
        "      <td>  0.4384292</td>\n",
        "      <td>-0.07462531</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_49</th>\n",
        "      <td> -0.01087431</td>\n",
        "      <td>  0.1425389</td>\n",
        "      <td>-0.07629013</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_50</th>\n",
        "      <td> -0.01412236</td>\n",
        "      <td>  0.1791519</td>\n",
        "      <td>  -0.078829</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_31</th>\n",
        "      <td> -0.01325812</td>\n",
        "      <td>  0.1679397</td>\n",
        "      <td>-0.07894569</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_1</th>\n",
        "      <td> -0.01239387</td>\n",
        "      <td>  0.1558131</td>\n",
        "      <td>-0.07954317</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>month_12</th>\n",
        "      <td> -0.02273634</td>\n",
        "      <td>  0.2820635</td>\n",
        "      <td>-0.08060717</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_36</th>\n",
        "      <td> -0.01574639</td>\n",
        "      <td>  0.1945981</td>\n",
        "      <td>-0.08091745</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weekday_2</th>\n",
        "      <td> -0.03409501</td>\n",
        "      <td>  0.3864051</td>\n",
        "      <td>-0.08823645</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>month_9</th>\n",
        "      <td> -0.03048607</td>\n",
        "      <td>  0.3438293</td>\n",
        "      <td>-0.08866628</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>text_proba</th>\n",
        "      <td> -0.01623458</td>\n",
        "      <td>  0.1757139</td>\n",
        "      <td> -0.0923921</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_26</th>\n",
        "      <td> -0.02009611</td>\n",
        "      <td>  0.2131797</td>\n",
        "      <td>-0.09426843</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>week_13</th>\n",
        "      <td> -0.01708549</td>\n",
        "      <td>  0.1620032</td>\n",
        "      <td> -0.1054639</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>day_5</th>\n",
        "      <td> -0.02544305</td>\n",
        "      <td>  0.2175279</td>\n",
        "      <td> -0.1169645</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hour_5</th>\n",
        "      <td> -0.02785534</td>\n",
        "      <td>  0.2087203</td>\n",
        "      <td> -0.1334577</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>latenight</th>\n",
        "      <td>           0</td>\n",
        "      <td>          0</td>\n",
        "      <td>        NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>174 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "                                                        mean         std  \\\n",
        "week_4                                            0.00982962           0   \n",
        "request_len                                         114.7274    259.8318   \n",
        "body_nwords                                          21.7493    50.36154   \n",
        "requester_upvotes_plus_downvotes_at_request         2533.101    6100.875   \n",
        "upvotes                                             1497.989    3628.885   \n",
        "downvotes                                           1035.112    2560.231   \n",
        "body_sents                                          1.204266    3.354955   \n",
        "requester_upvotes_minus_downvotes_at_request        462.8775    1492.145   \n",
        "money_category                                      1.158471    4.425892   \n",
        "job_category                                       0.2453701    1.102057   \n",
        "desire_category                                    0.1462097   0.6654951   \n",
        "family_category                                    0.1807036   0.8623809   \n",
        "week_6                                            0.01606929  0.07864736   \n",
        "week_16                                           0.01475868  0.07864736   \n",
        "day_18                                            0.02220449   0.1196342   \n",
        "requester_number_of_comments_in_raop_at_request     0.450662    2.468582   \n",
        "excl_marks_body                                    0.1470074   0.8623958   \n",
        "week_22                                           0.01530951   0.0907195   \n",
        "week_26                                            0.0262693   0.1558131   \n",
        "hour_8                                           0.007103919  0.04550158   \n",
        "morning                                           0.02288829   0.1620032   \n",
        "has_tldr                                          0.01082683  0.07864736   \n",
        "month_6                                           0.03710563   0.2758892   \n",
        "hour_1                                             0.0300492   0.2259164   \n",
        "day_30                                            0.02037153   0.1558131   \n",
        "requester_number_of_posts_on_raop_at_request      0.03880563    0.302605   \n",
        "dollars_body                                      0.04633692   0.3711641   \n",
        "zero_subreddits                                   0.04015423   0.3325248   \n",
        "day_28                                            0.01609778   0.1353672   \n",
        "week_44                                           0.01072236   0.0907195   \n",
        "...                                                      ...         ...   \n",
        "title_nwords                                      -0.4484966     7.90846   \n",
        "week_34                                          -0.01019051   0.1791519   \n",
        "week_45                                         -0.007388835   0.1277599   \n",
        "month_5                                          -0.01597432   0.2758892   \n",
        "first_half_of_month                              -0.02994473    0.500277   \n",
        "weekday_3                                        -0.02256539   0.3683645   \n",
        "afternoon                                        -0.01835812   0.2880458   \n",
        "title_len                                          -2.559557    39.99822   \n",
        "day_4                                            -0.01246985   0.1945981   \n",
        "week_18                                         -0.009668167    0.149338   \n",
        "holiday                                          -0.01226091   0.1844677   \n",
        "week_40                                          -0.01129219   0.1679397   \n",
        "hour_18                                          -0.01859555   0.2758892   \n",
        "day_1                                            -0.01571789   0.2217715   \n",
        "hour_14                                          -0.01281175   0.1791519   \n",
        "evening                                          -0.03271791   0.4384292   \n",
        "week_49                                          -0.01087431   0.1425389   \n",
        "week_50                                          -0.01412236   0.1791519   \n",
        "day_31                                           -0.01325812   0.1679397   \n",
        "week_1                                           -0.01239387   0.1558131   \n",
        "month_12                                         -0.02273634   0.2820635   \n",
        "week_36                                          -0.01574639   0.1945981   \n",
        "weekday_2                                        -0.03409501   0.3864051   \n",
        "month_9                                          -0.03048607   0.3438293   \n",
        "text_proba                                       -0.01623458   0.1757139   \n",
        "day_26                                           -0.02009611   0.2131797   \n",
        "week_13                                          -0.01708549   0.1620032   \n",
        "day_5                                            -0.02544305   0.2175279   \n",
        "hour_5                                           -0.02785534   0.2087203   \n",
        "latenight                                                  0           0   \n",
        "\n",
        "                                                mean_div_std  \n",
        "week_4                                                   inf  \n",
        "request_len                                        0.4415451  \n",
        "body_nwords                                        0.4318633  \n",
        "requester_upvotes_plus_downvotes_at_request         0.415203  \n",
        "upvotes                                            0.4127961  \n",
        "downvotes                                          0.4043042  \n",
        "body_sents                                         0.3589515  \n",
        "requester_upvotes_minus_downvotes_at_request       0.3102095  \n",
        "money_category                                     0.2617485  \n",
        "job_category                                       0.2226473  \n",
        "desire_category                                    0.2197006  \n",
        "family_category                                    0.2095403  \n",
        "week_6                                             0.2043208  \n",
        "week_16                                            0.1876563  \n",
        "day_18                                             0.1856032  \n",
        "requester_number_of_comments_in_raop_at_request     0.182559  \n",
        "excl_marks_body                                     0.170464  \n",
        "week_22                                            0.1687566  \n",
        "week_26                                            0.1685949  \n",
        "hour_8                                             0.1561247  \n",
        "morning                                             0.141283  \n",
        "has_tldr                                           0.1376629  \n",
        "month_6                                            0.1344947  \n",
        "hour_1                                             0.1330103  \n",
        "day_30                                             0.1307434  \n",
        "requester_number_of_posts_on_raop_at_request       0.1282385  \n",
        "dollars_body                                       0.1248422  \n",
        "zero_subreddits                                    0.1207556  \n",
        "day_28                                             0.1189194  \n",
        "week_44                                            0.1181924  \n",
        "...                                                      ...  \n",
        "title_nwords                                     -0.05671099  \n",
        "week_34                                          -0.05688199  \n",
        "week_45                                          -0.05783374  \n",
        "month_5                                           -0.0579012  \n",
        "first_half_of_month                              -0.05985629  \n",
        "weekday_3                                        -0.06125832  \n",
        "afternoon                                        -0.06373335  \n",
        "title_len                                        -0.06399178  \n",
        "day_4                                            -0.06407998  \n",
        "week_18                                          -0.06474018  \n",
        "holiday                                          -0.06646642  \n",
        "week_40                                          -0.06723956  \n",
        "hour_18                                          -0.06740223  \n",
        "day_1                                            -0.07087429  \n",
        "hour_14                                          -0.07151333  \n",
        "evening                                          -0.07462531  \n",
        "week_49                                          -0.07629013  \n",
        "week_50                                            -0.078829  \n",
        "day_31                                           -0.07894569  \n",
        "week_1                                           -0.07954317  \n",
        "month_12                                         -0.08060717  \n",
        "week_36                                          -0.08091745  \n",
        "weekday_2                                        -0.08823645  \n",
        "month_9                                          -0.08866628  \n",
        "text_proba                                        -0.0923921  \n",
        "day_26                                           -0.09426843  \n",
        "week_13                                           -0.1054639  \n",
        "day_5                                             -0.1169645  \n",
        "hour_5                                            -0.1334577  \n",
        "latenight                                                NaN  \n",
        "\n",
        "[174 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 4. Summary Table **  \n",
      "In order to get a sense of the contribution of each of our variables to our model, we show the coefficients from the final logistic regression on normalized input data. The size of the coefficient gives an indication of how impactful it was on prediction. From this, we can tell that the text vectorization process and the subreddit pca variable are extremely important, as well as the presene of an image or the number of posts a user made on reddit prior to requesting pizza.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_scaled = preprocessing.scale(train_data_subreddit_count.astype(float))\n",
      "logistic = LogisticRegression()\n",
      "logistic.fit(train_data_scaled, train_labels)\n",
      "\n",
      "coefs = pd.DataFrame(zip(train_data_subreddit_count.columns, np.transpose(logistic.coef_)))\n",
      "coefs.columns = ['Variable', 'Coefficient']\n",
      "coefs = coefs.sort(columns = 'Coefficient')\n",
      "\n",
      "def print_full(x):\n",
      "    pd.set_option('display.max_rows', len(x))\n",
      "    pd.set_option('display.max_colwidth', -1)\n",
      "    print(x)\n",
      "    pd.reset_option('display.max_colwidth')\n",
      "    pd.reset_option('display.max_rows')\n",
      "\n",
      "print_full(coefs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                               Variable           Coefficient\n",
        "173  text_proba                                          [-1.35020269949]    \n",
        "172  subreddit_proba                                     [-0.91747366165]    \n",
        "159  month_3                                             [-0.302099644594]   \n",
        "168  month_12                                            [-0.251328857426]   \n",
        "10   request_len                                         [-0.235253393962]   \n",
        "157  month_1                                             [-0.231610746157]   \n",
        "158  month_2                                             [-0.229497690597]   \n",
        "170  utcdiff_-3600.0                                     [-0.185443563767]   \n",
        "96   week_30                                             [-0.180191033839]   \n",
        "95   week_29                                             [-0.158056258333]   \n",
        "103  week_37                                             [-0.157369465471]   \n",
        "70   week_4                                              [-0.156654183382]   \n",
        "127  day_9                                               [-0.152247173661]   \n",
        "0    requester_account_age_in_days_at_request            [-0.130687626006]   \n",
        "148  day_30                                              [-0.127279777602]   \n",
        "11   zero_subreddits                                     [-0.124980569126]   \n",
        "112  week_46                                             [-0.123846027878]   \n",
        "169  utcdiff_-28800.0                                    [-0.123540326948]   \n",
        "166  month_10                                            [-0.122475984743]   \n",
        "104  week_38                                             [-0.112643367374]   \n",
        "167  month_11                                            [-0.106600446873]   \n",
        "94   week_28                                             [-0.096419331906]   \n",
        "114  week_48                                             [-0.0879412027046]  \n",
        "143  day_25                                              [-0.0841316586721]  \n",
        "75   week_9                                              [-0.0819668548206]  \n",
        "64   hour_21                                             [-0.0795918146726]  \n",
        "13   comment_ratio                                       [-0.0789639090989]  \n",
        "146  day_28                                              [-0.0749326426338]  \n",
        "25   title_nwords                                        [-0.073982908531]   \n",
        "131  day_13                                              [-0.0732054358507]  \n",
        "32   excl_marks_title                                    [-0.0687506475012]  \n",
        "51   hour_8                                              [-0.0644628351487]  \n",
        "141  day_23                                              [-0.0639991714192]  \n",
        "128  day_10                                              [-0.0620453830854]  \n",
        "24   holiday                                             [-0.057889836592]   \n",
        "53   hour_10                                             [-0.0564287497519]  \n",
        "99   week_33                                             [-0.0560372005985]  \n",
        "56   hour_13                                             [-0.0522181927567]  \n",
        "47   hour_4                                              [-0.0520705577918]  \n",
        "121  day_3                                               [-0.0519531719251]  \n",
        "101  week_35                                             [-0.0511729311858]  \n",
        "73   week_7                                              [-0.0506496596561]  \n",
        "123  day_5                                               [-0.0481521692524]  \n",
        "46   hour_3                                              [-0.0472708444487]  \n",
        "161  month_5                                             [-0.0437663141986]  \n",
        "113  week_47                                             [-0.0427759533983]  \n",
        "152  weekday_2                                           [-0.0418522379463]  \n",
        "147  day_29                                              [-0.0416563915256]  \n",
        "27   body_sents                                          [-0.0397684268528]  \n",
        "30   title_caps                                          [-0.039329743504]   \n",
        "91   week_25                                             [-0.0385882998167]  \n",
        "156  weekday_6                                           [-0.0383953651528]  \n",
        "102  week_36                                             [-0.0380823179368]  \n",
        "62   hour_19                                             [-0.0367408067397]  \n",
        "41   gratitude                                           [-0.0366672550365]  \n",
        "22   night                                               [-0.0357199286261]  \n",
        "15   downvotes                                           [-0.0353531849333]  \n",
        "43   hour_0                                              [-0.0336173188976]  \n",
        "4    requester_number_of_posts_at_request                [-0.0318561185973]  \n",
        "154  weekday_4                                           [-0.0296109153437]  \n",
        "63   hour_20                                             [-0.0291378853595]  \n",
        "44   hour_1                                              [-0.0284340897899]  \n",
        "42   reciprocity                                         [-0.0256387390834]  \n",
        "68   week_2                                              [-0.0245558219678]  \n",
        "151  weekday_1                                           [-0.0244920510457]  \n",
        "19   morning                                             [-0.0234975080475]  \n",
        "18   weekend                                             [-0.022989745609]   \n",
        "139  day_21                                              [-0.0217918365877]  \n",
        "97   week_31                                             [-0.021383572835]   \n",
        "119  day_1                                               [-0.0192830872823]  \n",
        "110  week_44                                             [-0.0184603695899]  \n",
        "111  week_45                                             [-0.0171323448289]  \n",
        "122  day_4                                               [-0.0162373905784]  \n",
        "136  day_18                                              [-0.0159723916567]  \n",
        "29   has_tldr                                            [-0.0144523239995]  \n",
        "90   week_24                                             [-0.0138713077767]  \n",
        "8    requester_upvotes_plus_downvotes_at_request         [-0.011995720793]   \n",
        "150  weekday_0                                           [-0.011660009561]   \n",
        "85   week_19                                             [-0.011334760068]   \n",
        "50   hour_7                                              [-0.00910462574923] \n",
        "93   week_27                                             [-0.00782175319731] \n",
        "69   week_3                                              [-0.00755129871006] \n",
        "2    requester_number_of_comments_at_request             [-0.00719265732279] \n",
        "60   hour_17                                             [-0.00674961776191] \n",
        "45   hour_2                                              [-0.00623093667257] \n",
        "120  day_2                                               [-0.0059438058851]  \n",
        "88   week_22                                             [-0.00558974577196] \n",
        "12   total_comments                                      [-0.00246027715546] \n",
        "80   week_14                                             [-0.00148000340796] \n",
        "116  week_50                                             [-0.000296188424927]\n",
        "23   latenight                                           [0.0]               \n",
        "145  day_27                                              [0.00477491031802]  \n",
        "33   excl_marks_body                                     [0.00519718063237]  \n",
        "54   hour_11                                             [0.00545482615467]  \n",
        "98   week_32                                             [0.00582120681455]  \n",
        "55   hour_12                                             [0.00594791402072]  \n",
        "14   upvotes                                             [0.00611295783607]  \n",
        "149  day_31                                              [0.00686718559085]  \n",
        "72   week_6                                              [0.00743427327424]  \n",
        "38   job_category                                        [0.00903508861053]  \n",
        "155  weekday_5                                           [0.00963676293001]  \n",
        "36   desire_category                                     [0.0104982430682]   \n",
        "34   dollars_title                                       [0.0133764565364]   \n",
        "52   hour_9                                              [0.0135471482843]   \n",
        "105  week_39                                             [0.0169046607251]   \n",
        "115  week_49                                             [0.0183460960536]   \n",
        "86   week_20                                             [0.0188221579308]   \n",
        "137  day_19                                              [0.020481833152]    \n",
        "83   week_17                                             [0.0208073221676]   \n",
        "117  week_51                                             [0.0225298539328]   \n",
        "31   body_caps                                           [0.0230923572241]   \n",
        "138  day_20                                              [0.0257619422606]   \n",
        "49   hour_6                                              [0.0265833495917]   \n",
        "125  day_7                                               [0.0281934342042]   \n",
        "135  day_17                                              [0.0288411516585]   \n",
        "71   week_5                                              [0.029165568136]    \n",
        "74   week_8                                              [0.0319326107467]   \n",
        "89   week_23                                             [0.0356512404888]   \n",
        "134  day_16                                              [0.036491901547]    \n",
        "142  day_24                                              [0.0367782843046]   \n",
        "17   first_half_of_month                                 [0.0386551226628]   \n",
        "6    requester_number_of_subreddits_at_request           [0.0422620583836]   \n",
        "58   hour_15                                             [0.0424462055071]   \n",
        "144  day_26                                              [0.0430797581583]   \n",
        "40   student_category                                    [0.043669434011]    \n",
        "124  day_6                                               [0.0455690857736]   \n",
        "48   hour_5                                              [0.0456044605081]   \n",
        "82   week_16                                             [0.0456870070586]   \n",
        "35   dollars_body                                        [0.0461729376388]   \n",
        "108  week_42                                             [0.0462780778027]   \n",
        "164  month_8                                             [0.046455358456]    \n",
        "21   evening                                             [0.0474182804932]   \n",
        "59   hour_16                                             [0.0480438215243]   \n",
        "65   hour_22                                             [0.0499199207163]   \n",
        "66   hour_23                                             [0.0500114474888]   \n",
        "126  day_8                                               [0.0540244669365]   \n",
        "1    requester_days_since_first_post_on_raop_at_request  [0.0562742446601]   \n",
        "81   week_15                                             [0.0568734280219]   \n",
        "20   afternoon                                           [0.060618941625]    \n",
        "37   family_category                                     [0.0666074632322]   \n",
        "87   week_21                                             [0.0692991004089]   \n",
        "106  week_40                                             [0.0724126357838]   \n",
        "129  day_11                                              [0.0743167716446]   \n",
        "16   upvote_ratio                                        [0.0744370829225]   \n",
        "78   week_12                                             [0.0785897565685]   \n",
        "61   hour_18                                             [0.0833562307795]   \n",
        "67   week_1                                              [0.085321851904]    \n",
        "107  week_41                                             [0.0860713904501]   \n",
        "26   body_nwords                                         [0.0861664827999]   \n",
        "109  week_43                                             [0.0865684213352]   \n",
        "118  week_52                                             [0.0899185482169]   \n",
        "100  week_34                                             [0.0900196183789]   \n",
        "76   week_10                                             [0.0913237134114]   \n",
        "57   hour_14                                             [0.0923062427816]   \n",
        "28   has_image                                           [0.0946878208651]   \n",
        "132  day_14                                              [0.0954775626864]   \n",
        "39   money_category                                      [0.0972238261874]   \n",
        "130  day_12                                              [0.09887967405]     \n",
        "160  month_4                                             [0.104464158453]    \n",
        "133  day_15                                              [0.105328980481]    \n",
        "9    title_len                                           [0.107748719511]    \n",
        "77   week_11                                             [0.118461383435]    \n",
        "140  day_22                                              [0.123713227376]    \n",
        "162  month_6                                             [0.130412148405]    \n",
        "153  weekday_3                                           [0.137184255697]    \n",
        "7    requester_upvotes_minus_downvotes_at_request        [0.139518184897]    \n",
        "92   week_26                                             [0.140599538624]    \n",
        "84   week_18                                             [0.142578405775]    \n",
        "165  month_9                                             [0.171810484917]    \n",
        "5    requester_number_of_posts_on_raop_at_request        [0.173544558085]    \n",
        "3    requester_number_of_comments_in_raop_at_request     [0.210211500604]    \n",
        "171  utcdiff_0.0                                         [0.22163005107]     \n",
        "79   week_13                                             [0.223445086429]    \n",
        "163  month_7                                             [0.437726729997]    \n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 5. Discussion  **  \n",
      "Our final result on Kaggle is an accuracy of 59.7%, good for 194th place at the time of this writing. In general, the theme with this competitions seems to be 'less is more.' We tried a lot of variations on the final recipe detailed above, but our additions seemed more likely to reduce our score than add to it. For instance, we tried stemming and lemmatizing the words, topic modelling with Latent Dirichlet Allocation, using tf-idf rather than simple counts in the word vectorizer, doc2vec, adjusting the regularization coefficients, adjusting the minimum and maximum word filters in the count vectorizer, but all of them seemed to hurt our scores. This does make some sense - we have a relatively small number of training observations, so we're prone to overfit our data.  \n",
      "\n",
      "On the modelling side, we found that many models, such as Naive Bayes, Support Vector Machines, and Random Forests, yielded an incredibly small number of successes. Since we had a small number of observations relative to the number of features we had, we would have liked to use these, but they proved unreliable to us in testing.  \n",
      "\n",
      "Our main successes proved to be the text features, which generally performed well, as well as the decision to crudely balance our dataset, which alone accounted for an approximately 1.5 percentage point increase in test accuracy. Aparently, balancing classes can be essential in the right situation. Additionally, using PCA to cut down on the number of subreddit and text features proved especially helpful.\n",
      "\n",
      "Given more time, we would like to investigate why classifiers other than logistic regression didn't work well for us and use mroe of an ensemble approach for the final prediction. We would balance classes in a more careful way than duplicating the successes. We would also try to explore topic modelling further. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 6. Appendix **  \n",
      "We include two methods that we attempted but which did not help us in our final result: running Latent Dirichlet Allocation on the nouns of each request, and attempting to create paragraph vectors for each request. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 6a. Latent Dirichlet Allocation **  \n",
      "We attempted to perform topic modelling on the requests via LDA, but ended up getting very strange topics back. Adding the topic weights into the final model hurt performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use the Stanford Part-Of-Speech Tagger to remove only the nouns from each request\n",
      "# Takes a long time, hence printing the progress\n",
      "\n",
      "def get_nouns(request_text):\n",
      "    noun_docs = []\n",
      "    counter = 0\n",
      "    for i in request_text.loc[:,'request_text_edit_aware']:\n",
      "        counter += 1\n",
      "        if counter % 500 == 0:\n",
      "            print counter\n",
      "        document = []\n",
      "        words = pos_tag(word_tokenize(i))\n",
      "        for j in words:\n",
      "            (word, pos) = j\n",
      "            if pos == 'NN':\n",
      "                document.append(word.encode('utf-8'))\n",
      "        noun_docs.append(' '.join(document))\n",
      "    return(noun_docs)\n",
      "\n",
      "train_nouns = get_nouns(train_request_text)\n",
      "test_nouns = get_nouns(test_request_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "500\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Feed the nouns into LDA for topic modelling\n",
      "vectorizer = CountVectorizer(stop_words = 'english')\n",
      "train_noun_count_vectors = vectorizer.fit_transform(train_nouns)\n",
      "\n",
      "vocab = [i.encode(\"utf8\") for i in vectorizer.vocabulary_]\n",
      "model = lda.LDA(n_topics=10, n_iter=10000, random_state=1)\n",
      "train_doc_props = model.fit_transform(train_noun_count_vectors.toarray())\n",
      "\n",
      "### Print out topics\n",
      "topic_word = model.topic_word_\n",
      "n_top_words = 15\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
      "\n",
      "test_noun_count_vectors = vectorizer.transform(test_nouns)\n",
      "test_doc_props = model.transform(test_noun_count_vectors.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:lda:all zero row in document-term matrix found\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:lda:all zero row in document-term matrix found\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:lda:all zero column in document-term matrix found\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 0: extravagant gutter southern government implementation downward dispute meter rape assistant scrubby reaking geez trend\n",
        "Topic 1: government sore extravagant implementation southern mana limit geez meter moola hitch fear burrito arent\n",
        "Topic 2: southern sore loss mana anon government gutter burrito moola ip abundance society extravagant feta\n",
        "Topic 3: southern sore grind microphone subscriber kno puree stenosis mana gutter slight attend expense implementation\n",
        "Topic 4: southern extravagant implementation gutter microphone fear society start moola benifit deaf greener mana government\n",
        "Topic 5: southern burrito meter rape extravagant limit mana implementation inbox benifit geez secretary radio different\n",
        "Topic 6: southern gummi ripe december implementation assistant danger throat microphone haircut ignore lieu highway fifteenth\n",
        "Topic 7: wednesday leave extravagant beater southern implementation attend meter gutter rape moola fear benifit tux\n",
        "Topic 8: southern extravagant gutter implementation benifit fear pysiotherapist inbox celebratory mana anybody sponsor subscriber nightmare\n",
        "Topic 9: southern seperate implementation ripe government sore gutter extravagant mana reaking arent downward faith portfolio\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Add LDA output to other features, refit model\n",
      "train_data_lda = pd.concat([train_data_subreddit_count, pd.DataFrame(train_doc_props)],axis=1)\n",
      "test_data_lda = pd.concat([test_data_subreddit_count, pd.DataFrame(test_doc_props)],axis=1)\n",
      "\n",
      "## Try out full dataset\n",
      "logistic = LogisticRegression(penalty = \"l1\", C = 10)\n",
      "scores = cross_validation.cross_val_score(logistic, train_data_lda, train_labels, cv=10)\n",
      "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.7661 (+/- 0.0320)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** 6b. Doc2Vec **  \n",
      "Using the code from this blog post (http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis) as a guide, we attempted to leverage the Doc2Vec method invented at Google, which combines two models: Distributed Memory (DM) and Distributed Bag of Words (DBOW). Unfortunately, this method did not help improve the final score. Also, we likely had far too little training data in order to use this method in eargnest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### The following sections add in the Paragraph Vectorization method\n",
      "\n",
      "import gensim\n",
      "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "## Read in data\n",
      "def readjson(filename):\n",
      "    with open(filename) as infile:\n",
      "        data = json.load(infile)\n",
      "    return data\n",
      "\n",
      "train = readjson(\"train.json\")\n",
      "test = readjson(\"test.json\")\n",
      "\n",
      "train_df = pd.DataFrame(train)\n",
      "test_df = pd.DataFrame(test)\n",
      "train_request_text = train_df[['request_text_edit_aware']]\n",
      "test_request_text = test_df[['request_text_edit_aware']]\n",
      "\n",
      "train_pos = train_df.loc[train_df.requester_received_pizza == 1, 'request_text']\n",
      "train_pos2 = [x.encode('ascii','ignore') for x in train_pos.values.tolist() if x != '']\n",
      "\n",
      "train_neg = train_df.loc[train_df.requester_received_pizza == 0, 'request_text']\n",
      "train_neg2 = [x.encode('ascii','ignore') for x in train_neg.values.tolist() if x != '']\n",
      "\n",
      "#use 1 for positive sentiment, 0 for negative\n",
      "y = np.concatenate((np.ones(len(train_pos2)), np.zeros(len(train_neg2))))\n",
      "\n",
      "random.seed(10)\n",
      "x_train, x_test, y_train, y_test = train_test_split(np.concatenate((train_pos2, train_neg2)), y, test_size=0.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Do some very minor text preprocessing\n",
      "def cleanText(corpus):\n",
      "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
      "    corpus = [z.lower().replace('\\n','') for z in corpus]\n",
      "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
      "\n",
      "    #treat punctuation as individual words\n",
      "    for c in punctuation:\n",
      "        corpus = [z.replace(c, ' %s '%c) for z in corpus]\n",
      "    corpus = [z.split() for z in corpus]\n",
      "    return corpus\n",
      "\n",
      "x_train2 = cleanText(x_train)\n",
      "x_test2 = cleanText(x_test)\n",
      "\n",
      "#Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
      "#We do this by using the LabeledSentence method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
      "#a dummy index of the review.\n",
      "def labelizeReviews(reviews, label_type):\n",
      "    labelized = []\n",
      "    for i,v in enumerate(reviews):\n",
      "#         print i\n",
      "        label = '%s_%s'%(label_type,i)\n",
      "        labelized.append(LabeledSentence(v, [label]))\n",
      "    return labelized\n",
      "\n",
      "x_train3 = labelizeReviews(x_train2, 'TRAIN')\n",
      "x_test3 = labelizeReviews(x_test2, 'TEST')\n",
      "\n",
      "size = 400\n",
      "\n",
      "#instantiate our DM and DBOW models\n",
      "model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)\n",
      "model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, dm=0, workers=3)\n",
      "\n",
      "#build vocab over all reviews\n",
      "model_dm.build_vocab(np.concatenate((x_train3, x_test3)))\n",
      "model_dbow.build_vocab(np.concatenate((x_train3, x_test3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We pass through the data set multiple times, shuffling the training reviews each time to improve accuracy.\n",
      "all_train_reviews = np.array(x_train3)\n",
      "\n",
      "for epoch in range(10):\n",
      "    perm = np.random.permutation(all_train_reviews.shape[0])\n",
      "    model_dm.train(all_train_reviews[perm])\n",
      "    model_dbow.train(all_train_reviews[perm])\n",
      "\n",
      "#Get training set vectors from our models\n",
      "def getVecs(model, corpus, size):\n",
      "    vecs = [np.array(model[z.labels[0]]).reshape((1, size)) for z in corpus]\n",
      "    return np.concatenate(vecs)\n",
      "\n",
      "train_vecs_dm = getVecs(model_dm, x_train3, size)\n",
      "train_vecs_dbow = getVecs(model_dbow, x_train3, size)\n",
      "\n",
      "train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train over test set\n",
      "x_test3 = np.array(x_test3)\n",
      "\n",
      "for epoch in range(10):\n",
      "    perm = np.random.permutation(x_test3.shape[0])\n",
      "    model_dm.train(x_test3[perm])\n",
      "    model_dbow.train(x_test3[perm])\n",
      "\n",
      "#Construct vectors for test reviews\n",
      "test_vecs_dm = getVecs(model_dm, x_test3, size)\n",
      "test_vecs_dbow = getVecs(model_dbow, x_test3, size)\n",
      "\n",
      "test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "lr = SGDClassifier(loss='log', penalty='l1')\n",
      "lr.fit(train_vecs, y_train)\n",
      "\n",
      "print 'Train Accuracy: %.2f' % lr.score(train_vecs, y_train)\n",
      "print 'Test Accuracy: %.2f' % lr.score(test_vecs, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Accuracy: 0.70\n",
        "Test Accuracy: 0.56\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create ROC curve\n",
      "from sklearn.metrics import roc_curve, auc\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "pred_probas = lr.predict_proba(test_vecs)[:,1]\n",
      "\n",
      "fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
      "roc_auc = auc(fpr,tpr)\n",
      "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
      "plt.plot([0, 1], [0, 1], 'k--')\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.legend(loc='lower right')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIB6P8BwVLZhSqqPdcAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x12f43e290>"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}